{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image \n",
    "from PIL import Image \n",
    "import pickle\n",
    "import dlib\n",
    "from tkinter import *\n",
    "from tkinter import ttk  \n",
    "import tkinter as tk\n",
    "import os\n",
    "from os import listdir\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import win32com.client as win\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import savez_compressed\n",
    "from numpy import reshape\n",
    "from numpy import max\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_facenet import FaceNet\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "\"\"\"\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv2D,Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\"\"\"\n",
    "from mtcnn_cv2 import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect + take pic\n",
    "def new_user():\n",
    "    image_path = 'Data/images/'\n",
    "    detector = MTCNN()\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    name = input('enter user :')\n",
    "    count = 0\n",
    "    id_rand = random.sample(range(1,10000),1)\n",
    "    myList = os.listdir(image_path)\n",
    "\n",
    "    for line in myList:\n",
    "        Type = line.split(\".\")\n",
    "        split = os.path.splitext(line)\n",
    "        if len(Type) > 2:\n",
    "            y = Type[1]\n",
    "        elif len(split) >= 2:\n",
    "            y = split[1]\n",
    "        if id_rand == y:\n",
    "            id_rand = random.sample(range(1,10000),1)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        new_path = image_path+str(name)+\".\"+ str(id_rand)\n",
    "        col = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pixels = np.asarray(col)  \n",
    "        detections = detector.detect_faces(pixels)\n",
    "        for detection in detections:\n",
    "            score = detection[\"confidence\"]\n",
    "            if score > 0.90:\n",
    "                x, y, w, h = detection[\"box\"]\n",
    "                count+=1 \n",
    "                if not os.path.exists(new_path):\n",
    "                    os.makedirs(new_path)\n",
    "                cv2.imwrite(new_path+\"/\"+str(name)+\".\"+str(count)+ \".jpg\", col[y-20:y+h+20, x-20:x+w+20])    \n",
    "                cv2.waitKey(100)\n",
    "                if count == 20:\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "\n",
    "        cv2.imshow('cam',frame)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_training\n",
    "#Method to extract Face\n",
    "def extract_image(image):\n",
    "    img1 = PIL.Image.open(image)            #open the image\n",
    "    img1 = img1.convert('RGB')          #convert the image to RGB format \n",
    "    pixels = asarray(img1)              #convert the image to numpy array\n",
    "    detector = MTCNN()                  #assign the MTCNN detector\n",
    "    f = detector.detect_faces(pixels)\n",
    "    #fetching the (x,y)co-ordinate and (width-->w, height-->h) of the image\n",
    "    x1,y1,w,h = f[0]['box']\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2 = abs(x1+w)\n",
    "    y2 = abs(y1+h)\n",
    "    #locate the co-ordinates of face in the image\n",
    "    store_face = pixels[y1:y2,x1:x2]  \n",
    "    image1 = PIL.Image.fromarray(store_face,'RGB')    #convert the numpy array to object\n",
    "    image1 = image1.resize((160,160))             #resize the image\n",
    "    face_array = asarray(image1)                  #image to array\n",
    "    return face_array\n",
    "\n",
    "#Method to fetch the face\n",
    "def load_faces(directory):\n",
    "    face = []\n",
    "    i=1\n",
    "    for filename in listdir(directory):\n",
    "        if filename.endswith(\"png\") or filename.endswith(\"jpg\") or filename.endswith(\"jpeg\"): \n",
    "            path = directory + filename\n",
    "            faces = extract_image(path)\n",
    "            face.append(faces)\n",
    "    return face\n",
    "\n",
    "#Method to get the array of face data(trainX) and it's labels(trainY)\n",
    "def load_dataset(directory):\n",
    "    x, y = [],[]\n",
    "    i=1\n",
    "    for subdir in listdir(directory):\n",
    "        path = directory + subdir + '/'\n",
    "        #load all faces in subdirectory\n",
    "        faces = load_faces(path)\n",
    "        #create labels\n",
    "        labels = [subdir for _ in range(len(faces))]\n",
    "        #summarize\n",
    "        print(\"%d There are %d images in the class %s:\"%(i,len(faces),subdir))\n",
    "        x.extend(faces)\n",
    "        y.extend(labels)\n",
    "        i=i+1\n",
    "    return asarray(x),asarray(y)  \n",
    "def training(path):\n",
    "    #load the datasets\n",
    "    trainX,trainY = load_dataset(path)\n",
    "    try:\n",
    "        print('\\033[1m' + str(trainX.shape + trainY.shape))\n",
    "    except:\n",
    "        print('\\033[1m' + 'problem with shape')\n",
    "    #compress the data\n",
    "    savez_compressed('data/trainer/facenet-mtcnn-dataset.npz',trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding\n",
    "def embedding():\n",
    "    #load the compressed dataset and facenet keras model\n",
    "    data = load('data/trainer/facenet-mtcnn-dataset.npz')\n",
    "    trainx, trainy = data['arr_0'],data['arr_1']\n",
    "    print(trainx.shape, trainy.shape)\n",
    "\n",
    "    model = FaceNet()\n",
    "    #get the face embeddings\n",
    "    new_trainx = list()\n",
    "    embeddings = model.embeddings(trainx)\n",
    "    new_trainx.append(embeddings)\n",
    "    new_trainx = asarray(new_trainx)             #convert the embeddings into numpy array\n",
    "    print(new_trainx.shape, trainy.shape)\n",
    "\n",
    "    #compress the 128 embeddings of each face \n",
    "    savez_compressed('data/trainer/facenet-mtcnn-embeddings.npz',new_trainx,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recognizer\n",
    "def recognize():\n",
    "    detector = MTCNN()\n",
    "    #detector= FaceRecognition()\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    #cap.set(3, 1024) # set video widht\n",
    "    #cap.set(4, 720)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        detector = MTCNN()\n",
    "        model = FaceNet()\n",
    "        img1 = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pixels = np.asarray(img1)\n",
    "        detections = detector.detect_faces(pixels)\n",
    "        for detection in detections:\n",
    "            x, y, w, h = detection[\"box\"]\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),1)\n",
    "            store_face = pixels[y:y+h,x:x+w]\n",
    "            image1 = PIL.Image.fromarray(store_face,'RGB')\n",
    "            store_f = image1.resize((160,160), PIL.Image.ANTIALIAS)\n",
    "            face_array = np.asarray(store_f)\n",
    "            #face_array = face_array.reshape(-1,160,160,3)     \n",
    "            face_array = np.expand_dims(face_array, axis = 0)\n",
    "            #print(\"Input test data shape: \",face_array.shape)\n",
    "\n",
    "            new_testx = list()\n",
    "            embeddings = model.embeddings(face_array)\n",
    "            new_testx.append(embeddings)\n",
    "            new_testx = asarray(new_testx)  \n",
    "            #print(\"Input test embedding shape: \",new_testx.shape)\n",
    "\n",
    "            data1 = load('data/trainer/facenet-mtcnn-dataset.npz')\n",
    "            train_x,train_y = data1['arr_0'],data1['arr_1']\n",
    "\n",
    "            data = load('data/trainer/facenet-mtcnn-embeddings.npz')\n",
    "            trainx,trainy= data['arr_0'],data['arr_1']\n",
    "            #print(\"Loaded data: Train=%d , Test=%d\"%(trainx.shape[0],new_testx.shape[0]))      \n",
    "\n",
    "            nsamples, nx, ny = trainx.shape\n",
    "            d2_train_x = trainx.reshape((nsamples*nx,ny))\n",
    "            nsamples, nx, ny = new_testx.shape\n",
    "            d2_train_n = new_testx.reshape((nsamples*nx,ny))\n",
    "            #print(d2_train_x.shape, d2_train_n.shape)\n",
    "\n",
    "            #normalize the input data\n",
    "            in_encode = Normalizer(norm='l2')\n",
    "            trainx = in_encode.transform(d2_train_x)\n",
    "            new_testx = in_encode.transform(d2_train_n)\n",
    "\n",
    "            #create a label vector\n",
    "            new_testy = trainy \n",
    "            out_encode = LabelEncoder()\n",
    "            out_encode.fit(trainy)\n",
    "            trainy = out_encode.transform(trainy)\n",
    "            new_testy = out_encode.transform(new_testy)\n",
    "\n",
    "            #define svm classifier model \n",
    "            model =SVC(kernel='linear', probability=True)\n",
    "            model.fit(trainx,trainy)\n",
    "\n",
    "            #predict\n",
    "            predict_train = model.predict(trainx)\n",
    "            predict_test = model.predict(new_testx)\n",
    "\n",
    "            #get the confidence score\n",
    "            probability = model.predict_proba(new_testx)\n",
    "            confidence = max(probability)\n",
    "\n",
    "            #Accuracy\n",
    "            acc_train = accuracy_score(trainy,predict_train)\n",
    "\n",
    "            #display\n",
    "            trainy_list = list(trainy)\n",
    "            p=int(predict_test)\n",
    "            if p in trainy_list:\n",
    "                val = trainy_list.index(p)\n",
    "\n",
    "            predict_test = out_encode.inverse_transform(predict_test)\n",
    "            #cv2.putText(frame, predict_test, (x, y), font, 0.5,(255,255,255),1,cv2.LINE_AA)\n",
    "            #cv2.putText(frame, train_x[val], (x, y), font, 0.5,(255,255,255),1,cv2.LINE_AA)\n",
    "            trainy = out_encode.inverse_transform(trainy)\n",
    "            #cv2.putText(frame, trainy[val], (x, y), font, 0.5,(255,255,255),1,cv2.LINE_AA)\n",
    "\n",
    "            name = trainy[val].split(\".\")[0]\n",
    "            print(confidence)\n",
    "            print(name)\n",
    "            speak=win.Dispatch(\"SAPI.spVoice\")\n",
    "            speak.rate=-1\n",
    "            #speak.speak(name)\n",
    "            color = (127,63,0)\n",
    "            cv2.putText(frame,  name+'/'+str(\"{:.2f}\".format(confidence)), (int(x+w+15), int(y-64)), font, 1, color, 2)\n",
    "            #cv2.putText(frame,  name, (x, y), font, 1, color, 2)\n",
    "            #connect face and text\n",
    "            cv2.line(frame,(x+w, y-64),(x+w-25, y-64),color,1)\n",
    "            cv2.line(frame,(int(x+w/2),y),(x+w-25,y-64),color,1)\n",
    "            \n",
    "            with open('data/attendance/Attendance_mtcnn+facenet.csv','r+') as f:\n",
    "                myDataList = f.readlines()\n",
    "                nameList = []\n",
    "                for line in myDataList:\n",
    "                    entry = line.split(',')\n",
    "                    nameList.append(entry[0])\n",
    "                if name not in nameList:\n",
    "                    now = datetime.now()\n",
    "                    dtString = now.strftime('%H:%M:%S')\n",
    "                    f.writelines(f'\\n{name},{dtString}')\n",
    "                    speak.speak(name)\n",
    "\n",
    "        cv2.imshow('cam', frame)\n",
    "        #plt.imshow(frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 13:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter user : wassim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-2-7941c5bb18c5>\", line 35, in new_user\n",
      "    cv2.imwrite(new_path+\"/\"+str(name)+\".\"+str(count)+ \".jpg\", col[y-20:y+h+20, x-20:x+w+20])\n",
      "cv2.error: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-71670poj\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:738: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 There are 1 images in the class Aaron_Guiel.[80711]:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-6-58709c103410>\", line 11, in <lambda>\n",
      "    b1 = ttk.Button(pop_up, text='Train model', command=lambda:[training(path)])\n",
      "  File \"<ipython-input-3-b526e8c95a94>\", line 50, in training\n",
      "    trainX,trainY = load_dataset(path)\n",
      "  File \"<ipython-input-3-b526e8c95a94>\", line 39, in load_dataset\n",
      "    faces = load_faces(path)\n",
      "  File \"<ipython-input-3-b526e8c95a94>\", line 28, in load_faces\n",
      "    faces = extract_image(path)\n",
      "  File \"<ipython-input-3-b526e8c95a94>\", line 10, in extract_image\n",
      "    x1,y1,w,h = f[0]['box']\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 There are 1 images in the class Aaron_Guiel.[80711]:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-6-58709c103410>\", line 11, in <lambda>\n",
      "    b1 = ttk.Button(pop_up, text='Train model', command=lambda:[training(path)])\n",
      "  File \"<ipython-input-3-b526e8c95a94>\", line 50, in training\n",
      "    trainX,trainY = load_dataset(path)\n",
      "  File \"<ipython-input-3-b526e8c95a94>\", line 39, in load_dataset\n",
      "    faces = load_faces(path)\n",
      "  File \"<ipython-input-3-b526e8c95a94>\", line 28, in load_faces\n",
      "    faces = extract_image(path)\n",
      "  File \"<ipython-input-3-b526e8c95a94>\", line 10, in extract_image\n",
      "    x1,y1,w,h = f[0]['box']\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "path =\"data/images/\"\n",
    "def F_popup():\n",
    "    pop_up = tk.Tk()\n",
    "    pop_up.wm_title('Facial Recognition')\n",
    "    pop_up.geometry(\"600x200\")\n",
    "    pop_up.grid_columnconfigure(0, weight=1)\n",
    "    pop_up.grid_rowconfigure(0, weight=1)\n",
    "    pop_up.grid_columnconfigure(1, weight=1)\n",
    "    pop_up.grid_rowconfigure(1, weight=1)\n",
    "    \n",
    "    b1 = ttk.Button(pop_up, text='Train model', command=lambda:[training(path)])\n",
    "    b1.grid(row=0, column=0, sticky=NSEW)\n",
    "    b2 = ttk.Button(pop_up, text='embedding', command=embedding)\n",
    "    b2.grid(row=0, column=1, sticky=NSEW)\n",
    "    b3 = ttk.Button(pop_up, text='Facial recognition', command=recognize)\n",
    "    b3.grid(row=1, column=0, sticky=NSEW)\n",
    "    b4 = ttk.Button(pop_up, text='Close', command=pop_up.destroy)\n",
    "    b4.grid(row=1, column=1, sticky=NSEW)\n",
    "    pop_up.mainloop()\n",
    "    \n",
    "pop_up = tk.Tk()\n",
    "pop_up.wm_title('Facial Recognition')\n",
    "pop_up.geometry(\"600x100\")\n",
    "b1 = ttk.Button(pop_up, text='New users', command=new_user)\n",
    "b1.pack(side='left',pady=10,fill='both', expand=True)\n",
    "b2 = ttk.Button(pop_up, text='Continue', command = lambda:[pop_up.destroy(),F_popup()])\n",
    "b2.pack(side='right', pady=10,fill='both', expand=True)\n",
    "pop_up.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
