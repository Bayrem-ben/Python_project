{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion model loaded\n",
      "Age model loaded\n",
      "Gender model loaded\n",
      "Facial attibute analysis models loaded in  6.531927824020386  seconds\n"
     ]
    }
   ],
   "source": [
    "#face detection mtcnn\n",
    "#from mtcnn import MTCNN\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import imutils\n",
    "from imutils import face_utils\n",
    "import time\n",
    "import pandas as pd\n",
    "from deepface import DeepFace\n",
    "from deepface.extendedmodels import Age\n",
    "from deepface.commons import functions, realtime, distance as dst\n",
    "\n",
    "#detector = MTCNN()\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 1024) # set video widht\n",
    "cap.set(4, 720)\n",
    "# Define min window size to be recognized as a face\n",
    "#minW = 0.1*cap.get(3)\n",
    "#minH = 0.1*cap.get(4)\n",
    "# Initializing the face detector and facial landmark predictor \n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"data/dlib/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "tic = time.time()\n",
    "emotion_model = DeepFace.build_model('Emotion')  \n",
    "print(\"Emotion model loaded\") \n",
    "age_model = DeepFace.build_model('Age')\n",
    "print(\"Age model loaded\")\n",
    "gender_model = DeepFace.build_model('Gender')\n",
    "print(\"Gender model loaded\")\n",
    "toc = time.time()\n",
    "print(\"Facial attibute analysis models loaded in \",toc-tic,\" seconds\")\n",
    "        \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    #frame = cv2.imread(\"data/11.jpg\")\n",
    "    #frame = imutils.resize(frame, width=500)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    \n",
    "    for face in faces:\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        # Creating an object in which we will store the detected facial landmarks\n",
    "        landmarks = predictor(gray, face)     \n",
    "        landmarks = face_utils.shape_to_np(landmarks)\n",
    "        #for (x, y) in landmarks:\n",
    "            #cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "            \n",
    "        #cv2.putText(frame, only_name+'/'+str(\"{:.2f}\".format(conf)), (x, y), font, 1, (0, 0 ,255), 2,cv2.LINE_AA)\n",
    "\n",
    "        \n",
    "        gray_img = functions.preprocess_face(frame, target_size = (48, 48), grayscale = True, enforce_detection = False)\n",
    "        emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "        emotion_predictions = emotion_model.predict(gray_img)[0,:]\n",
    "        sum_of_predictions = emotion_predictions.sum()                      \n",
    "        mood_items = []\n",
    "        for i in range(0, len(emotion_labels)):\n",
    "            mood_item = []\n",
    "            emotion_label = emotion_labels[i]\n",
    "            emotion_prediction = 100 * emotion_predictions[i] / sum_of_predictions\n",
    "            mood_item.append(emotion_label)\n",
    "            mood_item.append(emotion_prediction)\n",
    "            mood_items.append(mood_item)                   \n",
    "        emotion_df = pd.DataFrame(mood_items, columns = [\"emotion\", \"score\"])\n",
    "        emotion_df = emotion_df.sort_values(by = [\"score\"], ascending=False).reset_index(drop=True)\n",
    "        for index, instance in emotion_df.iterrows():\n",
    "            emotion_label = \"%s \" % (instance['emotion'])\n",
    "            emotion_score = instance['score']/100\n",
    "        cv2.putText(frame, emotion_label, (x+w, y-h), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "                                            \n",
    "        face_224 = functions.preprocess_face(frame, target_size = (224, 224),grayscale = False, enforce_detection = False)\n",
    "        age_predictions = age_model.predict(face_224)[0,:]\n",
    "        apparent_age = Age.findApparentAge(age_predictions)\n",
    "        gender_prediction = gender_model.predict(face_224)[0,:]\n",
    "        if np.argmax(gender_prediction) == 0:\n",
    "            gender = \"W\"\n",
    "        elif np.argmax(gender_prediction) == 1:\n",
    "            gender = \"M\"\n",
    "        analysis_report = str(int(apparent_age))+\" \"+gender\n",
    "        cv2.putText(frame, analysis_report, (x+w, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 111, 255), 2)\n",
    "\n",
    "                      \n",
    "    \"\"\"\n",
    "    col = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pixels = np.asarray(col)  \n",
    "    detections = detector.detect_faces(pixels)\n",
    "\n",
    "    for detection in detections:\n",
    "        score = detection[\"confidence\"]\n",
    "        if score > 0.90:\n",
    "            x, y, w, h = detection[\"box\"]\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    cv2.imshow('cam',frame)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -i IMAGE [-w WEIGHTS]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -i/--image\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#face detection dlib cnn\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# handle command line arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument('-i', '--image', required=True, help='path to image file')\n",
    "ap.add_argument('-w', '--weights', default='data/mmod_human_face_detector.dat',\n",
    "                help='path to weights file')\n",
    "args = ap.parse_args()\n",
    "\n",
    "# initialize hog + svm based face detector\n",
    "hog_face_detector = dlib.get_frontal_face_detector()\n",
    "# initialize cnn based face detector with the weights\n",
    "cnn_face_detector = dlib.cnn_face_detection_model_v1(args.weights)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 800) # set video widht\n",
    "cap.set(4, 600)\n",
    "# Define min window size to be recognized as a face\n",
    "#minW = 0.1*cap.get(3)\n",
    "#minH = 0.1*cap.get(4)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"data/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    #frame = imutils.resize(frame, width=500)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    start = time.time()\n",
    "    # apply face detection (cnn)\n",
    "    faces_cnn = cnn_face_detector(gray, 1)\n",
    "    end = time.time()\n",
    "    print(\"CNN : \", format(end - start, '.2f'))\n",
    "    # loop over detected faces\n",
    "    for face in faces_cnn:\n",
    "        x = face.rect.left()\n",
    "        y = face.rect.top()\n",
    "        w = face.rect.right() - x\n",
    "        h = face.rect.bottom() - y\n",
    "         # draw box over face\n",
    "        cv2.rectangle(image, (x,y), (x+w,y+h), (0,0,255), 2)\n",
    "    #cv2.putText(frame, only_name+'/'+str(\"{:.2f}\".format(conf)), (x, y), font, 1, (0, 0 ,255), 2,cv2.LINE_AA)\n",
    "        \n",
    "    cv2.imshow('cam',frame)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import cv2\n",
    "import dlib\n",
    "import argparse\n",
    "import time\n",
    "import easydict\n",
    "import sys\n",
    "from sys import argv\n",
    "\n",
    "\n",
    "\n",
    "# handle command line arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "#ap.add_argument('-i', '--image', required=True, help='path to image file')\n",
    "ap.add_argument('-w', '--weights', default='data/dlib/mmod_human_face_detector.dat', help='path to weights file')\n",
    "#args = ap.parse_args()\n",
    "\n",
    "args = ap.parse_args(sys.argv[1:])\n",
    "\n",
    "\n",
    "cnn_face_detector = dlib.cnn_face_detection_model_v1(args.weights)\n",
    "\n",
    "# load input image\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True :\n",
    "    ret, frame = cap.read()\n",
    "    #image = cv2.imread(args.image)\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    start = time.time()\n",
    "    # apply face detection (cnn)\n",
    "    faces_cnn = cnn_face_detector(image)\n",
    "    end = time.time()\n",
    "    print(\"CNN : \", format(end - start, '.2f'))\n",
    "\n",
    "    # loop over detected faces\n",
    "    for face in faces_cnn:\n",
    "        x = face.rect.left()\n",
    "        y = face.rect.top()\n",
    "        w = face.rect.right() - x\n",
    "        h = face.rect.bottom() - y\n",
    "        # draw box over face\n",
    "        cv2.rectangle(image, (x,y), (x+w,y+h), (0,0,255), 2)\n",
    "        #cv2.putText(image, \"CNN\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "    \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cv2.imshow(\"cnn_face_detection\", frame)\n",
    "# close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
